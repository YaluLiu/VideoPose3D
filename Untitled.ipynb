{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n",
    "import errno\n",
    "\n",
    "from common.camera import *\n",
    "from common.visualization import *\n",
    "from common.utils import *\n",
    "from common.generators import ChunkedGenerator, UnchunkedGenerator\n",
    "from common.h36m_dataset import Human36mDataset, preprocess_Human36m\n",
    "from common.visualization import *\n",
    "from common.model import *\n",
    "from common.xianhui_dataset import *\n",
    "import matplotlib\n",
    "import glob\n",
    "import plotly\n",
    "import json\n",
    "%matplotlib inline\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "from scripts.build_model import *\n",
    "from scripts.train import *\n",
    "from scripts.eval import *\n",
    "from scripts.data_preprocessing_cmu_mocap import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path = \"../wild_data_cmu/output_human/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " load 3d data \n",
      " processing 3d data \n",
      " load 2d keypoints \n",
      " processing 2d keypoints \n"
     ]
    }
   ],
   "source": [
    "dataset, keypoints = load_and_preprocess_cmu_mocap(data_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = sorted(dataset.subjects())\n",
    "subjects_train = subjects[:15]\n",
    "subjects_semi = subjects[15:23]\n",
    "subjects_test = subjects[23:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras_valid, poses_valid, poses_valid_2d = fetch(dataset = dataset,\n",
    "                                                   keypoints = keypoints, \n",
    "                                                   subjects = subjects_test, \n",
    "                                                   stride = 1)\n",
    "\n",
    "cameras_train, poses_train, poses_train_2d = fetch(dataset = dataset,\n",
    "                                                   keypoints = keypoints, \n",
    "                                                   subjects = subjects_train, \n",
    "                                                   stride = 1)\n",
    "\n",
    "cameras_semi, _, poses_semi_2d = fetch(dataset = dataset,\n",
    "                                       keypoints = keypoints,\n",
    "                                       subjects = subjects_semi,\n",
    "                                       stride = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoints/epoch_80.bin\n",
      "This model was trained for 80 epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume = True\n",
    "filter_widths = [3,3,3,3,3]\n",
    "model_pos_train, model_pos, model_traj, model_traj_train = build_models(17, 2, 17, filter_widths)\n",
    "\n",
    "# chk_filename = \"../checkpoint/pretrained_h36m_cpn.bin\"\n",
    "chk_filename = \"checkpoints/epoch_80.bin\"\n",
    "print('Loading checkpoint', chk_filename)\n",
    "checkpoint = torch.load(chk_filename, map_location=lambda storage, loc: storage)\n",
    "print('This model was trained for {} epochs'.format(checkpoint['epoch']))\n",
    "model_pos_train.load_state_dict(checkpoint['model_pos'])\n",
    "model_pos.load_state_dict(checkpoint['model_pos'])\n",
    "model_traj_train.load_state_dict(checkpoint['model_traj'])\n",
    "model_traj.load_state_dict(checkpoint['model_traj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Receptive field: 243 frames\n"
     ]
    }
   ],
   "source": [
    "receptive_field = model_pos.receptive_field()\n",
    "print('INFO: Receptive field: {} frames'.format(receptive_field))\n",
    "pad = (receptive_field - 1) // 2 # Padding on each side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = UnchunkedGenerator(cameras_valid[:1], \n",
    "                                    poses_valid[:1], \n",
    "                                    poses_valid_2d[:1], \n",
    "                                    pad=pad, \n",
    "                                    causal_shift=0, \n",
    "                                    augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for cam, batch, batch_2d in test_generator.next_epoch():\n",
    "    inputs_3d = torch.from_numpy(batch.astype('float32'))\n",
    "    inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "    cam = torch.from_numpy(cam.astype('float32'))\n",
    "    target = inputs_2d[:, pad:-pad, :, :2].contiguous()\n",
    "\n",
    "    if c > 2:\n",
    "        break\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_3d_cam = inputs_3d.clone()\n",
    "trajectory = inputs_3d[:,:,:1,:]\n",
    "input_3d_cam[:,:,1:,:] += trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pts to homo\n",
    "nframe = input_3d_cam[0].numpy().shape[0]\n",
    "pts_3d_cam_homo = np.ones((nframe, 17, 4))\n",
    "pts_3d_cam_homo[... , :3] = input_3d_cam[0].numpy()\n",
    "pts_3d_cam_homo = pts_3d_cam_homo.reshape(-1, 4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera to screen\n",
    "proj_2d = camera2screen(pts_3d_cam_homo[:3], dataset.cameras()[\"01\"][0][\"intrinsics\"].reshape(3,3))\n",
    "proj_2d = proj_2d.reshape(-1, 17, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_2d_normal = normalize_screen_coordinates(proj_2d[..., :2], \n",
    "                                       w=dataset.cameras()[\"01\"][0]['res_w'], \n",
    "                                       h=dataset.cameras()[\"01\"][0]['res_h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1618, 17, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7869874966051185e-12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.power(target.numpy()[0] - proj_2d_normal, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[316.27837911, 288.84749774],\n",
       "        [320.8831498 , 295.39540606],\n",
       "        [322.32539771, 318.97239582],\n",
       "        ...,\n",
       "        [327.55985877, 265.58752069],\n",
       "        [327.08234312, 292.07778584],\n",
       "        [327.06340056, 297.41170575]],\n",
       "\n",
       "       [[316.28363337, 288.84865445],\n",
       "        [320.89759464, 295.38955891],\n",
       "        [322.32648375, 318.96602944],\n",
       "        ...,\n",
       "        [327.56105675, 265.5882192 ],\n",
       "        [327.08055313, 292.07128147],\n",
       "        [327.06022159, 297.40146007]],\n",
       "\n",
       "       [[316.28858139, 288.84808202],\n",
       "        [320.91086909, 295.38308118],\n",
       "        [322.32581063, 318.95995217],\n",
       "        ...,\n",
       "        [327.56551435, 265.58380006],\n",
       "        [327.08846942, 292.07543901],\n",
       "        [327.06848866, 297.40955259]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[315.50595593, 289.45162848],\n",
       "        [320.045638  , 295.66635778],\n",
       "        [327.34291324, 317.77558798],\n",
       "        ...,\n",
       "        [323.53979776, 264.97679403],\n",
       "        [325.85379739, 291.38331217],\n",
       "        [326.95059554, 296.68853589]],\n",
       "\n",
       "       [[315.67867039, 289.45174032],\n",
       "        [320.21506947, 295.67154424],\n",
       "        [327.44351928, 317.82029771],\n",
       "        ...,\n",
       "        [323.68217357, 264.97012229],\n",
       "        [325.93456342, 291.40768445],\n",
       "        [327.01656248, 296.71878507]],\n",
       "\n",
       "       [[315.84466316, 289.43565073],\n",
       "        [320.4117064 , 295.63242042],\n",
       "        [327.53475682, 317.84845012],\n",
       "        ...,\n",
       "        [323.83751181, 264.97820037],\n",
       "        [326.04145568, 291.43834219],\n",
       "        [327.11053955, 296.75453467]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
